from pyspark.sql import SparkSession
from pyspark.sql.functions import col
# Create SparkSession
spark = SparkSession.builder \
    .appName("Top 10 Countries by Number of Customers") \
    .getOrCreate()
# Assuming you have a DataFrame named 'customers' with columns 'country' and 'customer_id'
# Replace 'customers' and 'customer_id' with actual DataFrame and column names in your dataset
# Read the data into DataFrame
customers = spark.read.csv("customers.csv", header=True)
# Group by country and count the number of customers
country_counts = customers.groupBy("country").count()
# Sort the counts in descending order
sorted_country_counts = country_counts.orderBy(col("count").desc())
# Take the top 10 countries
top_10_countries = sorted_country_counts.limit(10)
# Show the result
top_10_countries.show()
# Stop SparkSession
spark.stop()
